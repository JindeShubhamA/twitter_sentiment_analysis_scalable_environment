version: '3'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
    ports:
      - 9200:9200
    networks:
    - msgqueue
    healthcheck:
        test: ["CMD", "curl", "-f", "http://elasticsearch:9200"]
        interval: 30s
        timeout: 10s
        retries: 5

  spark-driver:
    build:
      context: ./src
      dockerfile: Dockerfile
    image: docker.io/richardswesterhof/mapreduce:latest
    hostname: spark-driver
    depends_on:
      - kafka
    links:
      - kafka
    networks:
      # join the external network for spark communication
      # this does mean the mapreduce cannot run without the spark cluster running in kubernetes
      # but this wouldn't really make sense anyway
      - msgqueue
      - default
    ports:
      - '30001:30001'
      - '30002:30002'

  spark-leader:
    image: docker.io/bitnami/spark:3.0.1
    hostname: spark-leader
    networks:
      - msgqueue
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8080:8080'
      - '7077:7077'

  spark-worker:
    image: docker.io/bitnami/spark:3.0.1
    # we want the leader to be done before doing the workers
    depends_on:
      - spark-leader
    deploy:
      replicas: 2
    networks:
      - msgqueue
#    ports:
#      - '8081:8081'
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-leader:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no

  producer:
    build:
      context: ./python_code
      dockerfile: Dockerfile_producer
    container_name: producer
    depends_on:
      - elasticsearch
      - kafka
    links:
      - elasticsearch
      - kafka
    restart: on-failure
    networks:
    - msgqueue

  consumer:
    build:
      context: ./python_code
      dockerfile: Dockerfile_consumer
    depends_on:
      - kafka
    links:
      - kafka
    container_name: consumer
    restart: on-failure
    networks:
    - msgqueue

  populate_db:
    build:
      context: ./python_code
      dockerfile: Dockerfile_populate
    container_name: populate_db
    networks:
      - msgqueue
    restart: on-failure

  zookeeper:
    image: wurstmeister/zookeeper:3.4.6
    ports:
      - "2181:2181"
    networks:
    - msgqueue

  kafka:
    build: python_code/kafka-docker/
    ports:
      - "9092:9092"
    expose:
      - "9093"
    container_name: kafka
    environment:
#      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "topic_test1:1:1"
    networks:
    - msgqueue
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
#
  client:
      image: appropriate/curl:latest
      networks:
        - msgqueue
      command: "curl http://elasticsearch:9200"
networks:
  msgqueue:
    driver: bridge